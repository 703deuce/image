# RunPod-native Dockerfile for FLUX.1-dev API
# Using RunPod's official PyTorch base image
# Build trigger: Updated to use standard attention without xformers
FROM runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Set essential environment variables for FLUX.1-dev
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
# Use network storage for model cache to avoid disk space issues
ENV HF_HOME=/runpod-volume/cache
ENV TRANSFORMERS_CACHE=/runpod-volume/cache
ENV DIFFUSERS_CACHE=/runpod-volume/cache
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
# HF_TOKEN will be set at runtime via RunPod environment variables

# Create cache directories (network storage will be mounted at /runpod-volume)
RUN mkdir -p /app/cache /runpod-volume/cache

# Install additional system dependencies if needed
RUN apt-get update && \
    apt-get install -y wget curl unzip git build-essential && \
    rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
# Force rebuild of pip install to pick up new requirements.txt changes
ARG CACHEBUST=1
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Install ostris/ai-toolkit manually with proper dependencies
# Force rebuild - change timestamp to bust Docker cache
ARG AI_TOOLKIT_REBUILD=2025-08-05-16-31
RUN echo "AI-Toolkit rebuild trigger: $AI_TOOLKIT_REBUILD" && \
    git clone https://github.com/ostris/ai-toolkit.git /app/ai-toolkit && \
    cd /app/ai-toolkit && \
    git submodule update --init --recursive && \
    pip install --no-cache-dir torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121 && \
    pip install --no-cache-dir -r requirements.txt && \
    ls -la /app/ai-toolkit/ && \
    echo "✅ ai-toolkit installation complete"

# Skip flash-attention - use standard attention for guaranteed compatibility
# This ensures faster builds and no dependency issues

# Copy application files
COPY handler.py runpod_api.py ./

# Pre-download FLUX.1-dev model during build to avoid runtime download issues
# This ensures the model is ready immediately when container starts
RUN python -c "
import os
from diffusers import FluxPipeline
import torch

# Ensure cache directory exists
os.makedirs('/runpod-volume/cache', exist_ok=True)

print('Pre-downloading FLUX.1-dev model...')
try:
    # Download model to network storage during build
    pipeline = FluxPipeline.from_pretrained(
        'black-forest-labs/FLUX.1-dev',
        torch_dtype=torch.bfloat16,
        cache_dir='/runpod-volume/cache'
    )
    print('✅ Model pre-downloaded successfully!')
except Exception as e:
    print(f'⚠️  Model pre-download failed: {e}')
    print('Model will be downloaded at runtime instead.')
"

# Simple health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import torch; exit(0 if torch.cuda.is_available() else 1)"

# Run the application
CMD ["python", "handler.py"]