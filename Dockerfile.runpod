# RunPod-native Dockerfile for FLUX.1-dev API
# Using RunPod's official PyTorch base image
# Build trigger: Updated to use standard attention without xformers
FROM runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Set essential environment variables for FLUX.1-dev
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
# Use network storage for model cache to avoid disk space issues
ENV HF_HOME=/runpod-volume/cache
ENV TRANSFORMERS_CACHE=/runpod-volume/cache
ENV DIFFUSERS_CACHE=/runpod-volume/cache
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
# HF_TOKEN will be set at runtime via RunPod environment variables

# Create cache directories (network storage will be mounted at /runpod-volume)
RUN mkdir -p /app/cache /runpod-volume/cache

# Install additional system dependencies if needed
RUN apt-get update && \
    apt-get install -y wget curl unzip git build-essential && \
    rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Skip flash-attention - use standard attention for guaranteed compatibility
# This ensures faster builds and no dependency issues

# Copy application files
COPY handler.py runpod_api.py ./

# Simple health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import torch; exit(0 if torch.cuda.is_available() else 1)"

# Run the application
CMD ["python", "handler.py"]