# RunPod-native Dockerfile for FLUX.1-dev API
# Using RunPod's official PyTorch base image
# Build trigger: Updated to use standard attention without xformers
FROM runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Set essential environment variables for FLUX.1-dev
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
# Use network storage for model cache to avoid disk space issues
ENV HF_HOME=/runpod-volume/cache
ENV TRANSFORMERS_CACHE=/runpod-volume/cache
ENV DIFFUSERS_CACHE=/runpod-volume/cache
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
# HF_TOKEN will be set at runtime via RunPod environment variables

# Create cache directories (network storage will be mounted at /runpod-volume)
RUN mkdir -p /app/cache /runpod-volume/cache

# Install additional system dependencies if needed
RUN apt-get update && \
    apt-get install -y wget curl unzip git build-essential && \
    rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Install ostris/ai-toolkit for proper FLUX LoRA training
ARG CACHEBUST=1
RUN git clone https://github.com/ostris/ai-toolkit.git /app/ai-toolkit && \
    cd /app/ai-toolkit && \
    pip install --no-cache-dir -r requirements.txt

# Copy ai-toolkit to volume at runtime (create a setup script)
RUN echo '#!/bin/bash\nif [ ! -d "/runpod-volume/ai-toolkit" ]; then\n  echo "üì¶ Copying ai-toolkit to persistent volume..."\n  cp -r /app/ai-toolkit /runpod-volume/ai-toolkit\n  echo "‚úÖ ai-toolkit copied to /runpod-volume/ai-toolkit"\nfi' > /app/setup-ai-toolkit.sh && \
    chmod +x /app/setup-ai-toolkit.sh

# Skip flash-attention - use standard attention for guaranteed compatibility
# This ensures faster builds and no dependency issues

# Copy application files
COPY handler.py runpod_api.py ./

# Pre-download FLUX.1-dev model during build to avoid runtime download issues
# This ensures the model is ready immediately when container starts
RUN python -c "
import os
from diffusers import FluxPipeline
import torch

# Ensure cache directory exists
os.makedirs('/runpod-volume/cache', exist_ok=True)

print('Pre-downloading FLUX.1-dev model...')
try:
    # Download model to network storage during build
    pipeline = FluxPipeline.from_pretrained(
        'black-forest-labs/FLUX.1-dev',
        torch_dtype=torch.bfloat16,
        cache_dir='/runpod-volume/cache'
    )
    print('‚úÖ Model pre-downloaded successfully!')
except Exception as e:
    print(f'‚ö†Ô∏è  Model pre-download failed: {e}')
    print('Model will be downloaded at runtime instead.')
"

# Simple health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import torch; exit(0 if torch.cuda.is_available() else 1)"

# Run the application
CMD ["python", "handler.py"]