# RunPod-native Dockerfile for FLUX.1-dev API
# Using RunPod's official PyTorch base image
# Build trigger: Updated to use standard attention without xformers
FROM runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Set essential environment variables for FLUX.1-dev
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
# Use network storage for model cache to avoid disk space issues
ENV HF_HOME=/runpod-volume/cache
ENV TRANSFORMERS_CACHE=/runpod-volume/cache
ENV DIFFUSERS_CACHE=/runpod-volume/cache
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
# HF_TOKEN will be set at runtime via RunPod environment variables

# Create cache directories (network storage will be mounted at /runpod-volume)
RUN mkdir -p /app/cache /runpod-volume/cache

# Install additional system dependencies if needed
RUN apt-get update && \
    apt-get install -y wget curl unzip git build-essential && \
    rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Skip flash-attention - use standard attention for guaranteed compatibility
# This ensures faster builds and no dependency issues

# Copy application files
COPY handler.py runpod_api.py ./

# Pre-download FLUX.1-dev model during build to local storage
# This ensures the model is ready immediately when container starts
RUN python -c "
import os
from diffusers import FluxPipeline
import torch

# Create local model cache directory
os.makedirs('/app/models', exist_ok=True)

print('Pre-downloading FLUX.1-dev model to local storage...')
try:
    # Download model to local storage during build (no HF_TOKEN needed for public access)
    pipeline = FluxPipeline.from_pretrained(
        'black-forest-labs/FLUX.1-dev',
        torch_dtype=torch.bfloat16,
        cache_dir='/app/models'
    )
    print('‚úÖ Model pre-downloaded successfully to /app/models!')
except Exception as e:
    print(f'‚ö†Ô∏è  Model pre-download failed: {e}')
    print('Model will be downloaded at runtime instead.')
"

# Create startup script to copy model to network volume at runtime
COPY <<EOF /app/startup.sh
#!/bin/bash
echo "üöÄ Starting FLUX.1-dev container..."

# Copy pre-downloaded model from local to network storage if available
if [ -d "/app/models" ] && [ -d "/runpod-volume" ]; then
    echo "üì¶ Copying pre-downloaded model to network storage..."
    mkdir -p /runpod-volume/cache
    cp -r /app/models/* /runpod-volume/cache/ 2>/dev/null || echo "‚ö†Ô∏è  Model copy failed, will download at runtime"
    echo "‚úÖ Model available in network storage"
fi

# Start the main application
exec python handler.py
EOF

RUN chmod +x /app/startup.sh

# Simple health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import torch; exit(0 if torch.cuda.is_available() else 1)"

# Run the startup script
CMD ["/app/startup.sh"]