# Optimized Dockerfile for FLUX.1-dev RunPod API  
# Using official NVIDIA CUDA base image (guaranteed to work)
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

# Set working directory
WORKDIR /app

# Set environment variables for optimization
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV HF_HOME=/app/cache
ENV TRANSFORMERS_CACHE=/app/cache  
ENV DIFFUSERS_CACHE=/app/cache
ENV PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Create cache directories
RUN mkdir -p /app/cache /tmp/outputs

# Install system dependencies including Python
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-dev \
    git \
    wget \
    curl \
    unzip \
    build-essential \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create python symlink for compatibility  
RUN ln -s /usr/bin/python3 /usr/bin/python

# Copy and install Python dependencies first (for better caching)
COPY requirements.txt .
RUN python3 -m pip install --no-cache-dir --upgrade pip setuptools wheel
RUN python3 -m pip install --no-cache-dir -r requirements.txt

# Install performance optimizations
RUN python3 -m pip install --no-cache-dir ninja flash-attn --no-build-isolation || echo "Optional packages failed"

# Copy application code
COPY handler.py runpod_api.py ./

# Pre-download model (optional - increases image size but reduces cold start)
# Uncomment the next line to pre-download the model (adds ~24GB to image size)
# RUN python -c "import torch; from diffusers import FluxPipeline; FluxPipeline.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.bfloat16)"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD python -c "import torch; print('CUDA:', torch.cuda.is_available())" || exit 1

# Set default command
CMD ["python", "handler.py"]
